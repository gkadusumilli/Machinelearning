{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "810c09f5-da70-4a77-af4d-ca4fcf0cf2dd",
   "metadata": {},
   "source": [
    "# QUANTIZATION - OVERVIEW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07110805-044e-4c0b-b589-9775f44a8b4a",
   "metadata": {},
   "source": [
    "## Quantization\n",
    "\n",
    "* What it achieves:\n",
    "  * Models are **4x smaller size**\n",
    "  * **1.5 - 4x** faster compute, lower power consumption\n",
    "  * Allows execution on specialized FP accelerators (Edge CPUs, TPUs, DSP)\n",
    "\n",
    "* How it works\n",
    "  * Reduce static parameter (weights) from **high precision to lower precision** (e.g. Float32 --> Float16)\n",
    "  * Executes operations between the static parameters and dynamic inputs, activation in lower precision\n",
    "  * Possible to run entire model computation in lower fixed point precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186a6e2a-5c48-4470-ae52-a83617fa1462",
   "metadata": {},
   "source": [
    "## Uniform/Linear quantization\n",
    "\n",
    "* WITHOUT QUANTIZATION\n",
    "  * TF Model --> tf.lite --> tflite model\n",
    "\n",
    "* WITH QUANTIZATION\n",
    "  * TF model --> tf.lite with quantization --> tflite model (More model compression & size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9200962f-1ab7-4df2-8ccf-45a81d7a8d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Oct 24 23:26:59 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.107.02             Driver Version: 550.107.02     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        Off |   00000000:01:00.0 Off |                  Off |\n",
      "|  0%   36C    P8             15W /  450W |      12MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      2171      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfe6d67b-7b8c-4a9c-8d63-654591c3d0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.8658 - loss: 0.4876 - val_accuracy: 0.9638 - val_loss: 0.1249\n",
      "Epoch 2/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9689 - loss: 0.1138 - val_accuracy: 0.9749 - val_loss: 0.0812\n",
      "Epoch 3/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9781 - loss: 0.0784 - val_accuracy: 0.9779 - val_loss: 0.0705\n",
      "Epoch 4/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9807 - loss: 0.0680 - val_accuracy: 0.9800 - val_loss: 0.0642\n",
      "Epoch 5/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9835 - loss: 0.0562 - val_accuracy: 0.9804 - val_loss: 0.0587\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x762e3fb096f0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.DEBUG)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers,models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Load MNIST dataset\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Normalize the input image so that each pixel value is between 0 to 1.\n",
    "train_images = train_images.astype(np.float32) / 255.0\n",
    "test_images = test_images.astype(np.float32) / 255.0\n",
    "\n",
    "# Define the model architecture\n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.InputLayer(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
    "  tf.keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "# Train the digit classification model\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "                  from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "model.fit(\n",
    "  train_images,\n",
    "  train_labels,\n",
    "  epochs=5,\n",
    "  validation_data=(test_images, test_labels)\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182f5d4f-daab-4b8e-a097-d0a0e2b1541c",
   "metadata": {},
   "source": [
    "#### Convert to a Tensorflow Lite model - No quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "032f8c9e-d112-4e89-8f6a-f6e13a22c7b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmps3w2ufem/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmps3w2ufem/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '/tmp/tmps3w2ufem'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 28, 28), dtype=tf.float32, name='keras_tensor_6')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  129941081825600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  129941082287872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  129941082285056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  129941082282768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1729792653.718877    5946 tf_tfl_flatbuffer_helpers.cc:392] Ignored output_format.\n",
      "W0000 00:00:1729792653.718886    5946 tf_tfl_flatbuffer_helpers.cc:395] Ignored drop_control_dependency.\n",
      "2024-10-24 23:27:33.719075: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmps3w2ufem\n",
      "2024-10-24 23:27:33.719264: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2024-10-24 23:27:33.719268: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmps3w2ufem\n",
      "2024-10-24 23:27:33.720798: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled\n",
      "2024-10-24 23:27:33.721062: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2024-10-24 23:27:33.732167: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmps3w2ufem\n",
      "2024-10-24 23:27:33.735274: I tensorflow/cc/saved_model/loader.cc:462] SavedModel load for tags { serve }; Status: success: OK. Took 16200 microseconds.\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca568a6-7678-4e9f-af2a-29d9c66524d4",
   "metadata": {},
   "source": [
    "#### Convert to a Tensorflow Lite model - Integer only quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2413f1de-9308-4bb9-bf47-c78fd10623c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp5pfyza3n/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp5pfyza3n/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '/tmp/tmp5pfyza3n'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 28, 28), dtype=tf.float32, name='keras_tensor_6')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  129941081825600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  129941082287872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  129941082285056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  129941082282768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gk/.local/lib/python3.10/site-packages/tensorflow/lite/python/convert.py:983: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n",
      "W0000 00:00:1729792657.106164    5946 tf_tfl_flatbuffer_helpers.cc:392] Ignored output_format.\n",
      "W0000 00:00:1729792657.106172    5946 tf_tfl_flatbuffer_helpers.cc:395] Ignored drop_control_dependency.\n",
      "2024-10-24 23:27:37.106260: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmp5pfyza3n\n",
      "2024-10-24 23:27:37.106460: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2024-10-24 23:27:37.106464: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmp5pfyza3n\n",
      "2024-10-24 23:27:37.107908: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2024-10-24 23:27:37.117166: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmp5pfyza3n\n",
      "2024-10-24 23:27:37.120255: I tensorflow/cc/saved_model/loader.cc:462] SavedModel load for tags { serve }; Status: success: OK. Took 13996 microseconds.\n",
      "2024-10-24 23:27:37.244831: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: UINT8, output_inference_type: UINT8\n"
     ]
    }
   ],
   "source": [
    "#Range of the values the model will encounter --> Uses TF dataset created from training images, batching them into single samples & taking 100 samples\n",
    "def representative_data_gen():\n",
    "  for input_value in tf.data.Dataset.from_tensor_slices(train_images).batch(1).take(100):\n",
    "    yield [input_value]\n",
    "\n",
    "#TF lite converter initialization\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "#Initialize weight quantization to reduce ther model size and improve inference speed\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "# Ensure that if any ops can't be quantized, the converter throws an error\n",
    "#INT8 quantization\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "#converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_FLOAT16]\n",
    "#converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
    "# Set the input and output tensors to uint8 (APIs added in r2.3)\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "#Convert TFlite model\n",
    "tflite_model_quant = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61760118-ae19-48dd-9ae5-7934abb2ce04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  <class 'numpy.uint8'>\n",
      "output:  <class 'numpy.uint8'>\n"
     ]
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(model_content=tflite_model_quant)\n",
    "input_type = interpreter.get_input_details()[0]['dtype']\n",
    "print('input: ', input_type)\n",
    "output_type = interpreter.get_output_details()[0]['dtype']\n",
    "print('output: ', output_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c1b04c-033a-4ff7-8306-93c6739d64dd",
   "metadata": {},
   "source": [
    "#### Save the TF Lite models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1645ca81-c847-4b25-a91a-5e284cdaff32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24800"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pathlib\n",
    "tflite_models_dir = pathlib.Path(\"./mnist_tflite_models/\")\n",
    "tflite_models_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Save the unquantized/float model:\n",
    "tflite_model_file = tflite_models_dir/\"mnist_model.tflite\"\n",
    "tflite_model_file.write_bytes(tflite_model)\n",
    "# Save the quantized model:\n",
    "tflite_model_quant_file = tflite_models_dir/\"mnist_model_quant.tflite\"\n",
    "tflite_model_quant_file.write_bytes(tflite_model_quant)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616c20ae-8c89-4993-866e-f90131c1053b",
   "metadata": {},
   "source": [
    "#### Run the TF Lite models\n",
    "\n",
    "TF interpreter: Interface for running TF Lite models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5474d1c-caee-40f2-b732-2ebdf5e5cfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Helper function to run inference on a TFLite model\n",
    "def run_tflite_model(tflite_file, test_image_indices):\n",
    "  global test_images\n",
    "\n",
    "  # Initialize the interpreter\n",
    "  interpreter = tf.lite.Interpreter(model_path=str(tflite_file))\n",
    "  interpreter.allocate_tensors()\n",
    "\n",
    "  input_details = interpreter.get_input_details()[0]\n",
    "  output_details = interpreter.get_output_details()[0]\n",
    "\n",
    "  predictions = np.zeros((len(test_image_indices),), dtype=int)\n",
    "  for i, test_image_index in enumerate(test_image_indices):\n",
    "    test_image = test_images[test_image_index]\n",
    "    test_label = test_labels[test_image_index]\n",
    "\n",
    "    # Check if the input type is quantized, then rescale input data to uint8\n",
    "    if input_details['dtype'] == np.uint8:\n",
    "      input_scale, input_zero_point = input_details[\"quantization\"]\n",
    "      test_image = test_image / input_scale + input_zero_point\n",
    "\n",
    "    test_image = np.expand_dims(test_image, axis=0).astype(input_details[\"dtype\"])\n",
    "    interpreter.set_tensor(input_details[\"index\"], test_image)\n",
    "    interpreter.invoke()\n",
    "    output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
    "\n",
    "    predictions[i] = output.argmax()\n",
    "\n",
    "  return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6f6203-8ac4-4616-86ab-1f5351343299",
   "metadata": {},
   "source": [
    "#### Test the models on one image\n",
    "\n",
    "* tflite_model file: Original TF lite model with floating point data\n",
    "* tflite_model_quant file: Integer only quantization (it uses uint8 data for input & output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88d2d811-3618-42f9-a8d4-d5d5df567a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to test a different image\n",
    "test_image_index = 1\n",
    "\n",
    "## Helper function to test the models on one image\n",
    "def test_model(tflite_file, test_image_index, model_type):\n",
    "  global test_labels\n",
    "\n",
    "  predictions = run_tflite_model(tflite_file, [test_image_index])\n",
    "\n",
    "  plt.imshow(test_images[test_image_index])\n",
    "  template = model_type + \" Model \\n True:{true}, Predicted:{predict}\"\n",
    "  _ = plt.title(template.format(true= str(test_labels[test_image_index]), predict=str(predictions[0])))\n",
    "  plt.grid(False)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b39b8fc-b0d8-46d5-abfc-4f46233b3aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_model(tflite_model_file, test_image_index, model_type=\"Float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3945b6fb-a2b1-4026-a324-876d4b122563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_model(tflite_model_quant_file, test_image_index, model_type=\"Quantized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1becdf-fc0e-44e3-9eac-edfad8f96235",
   "metadata": {},
   "source": [
    "#### Evaluate the models on all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99e40249-0c3d-47cc-9f1b-d883f1c5638c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to evaluate a TFLite model on all images\n",
    "def evaluate_model(tflite_file, model_type):\n",
    "  global test_images\n",
    "  global test_labels\n",
    "\n",
    "  test_image_indices = range(test_images.shape[0])\n",
    "  predictions = run_tflite_model(tflite_file, test_image_indices)\n",
    "\n",
    "  accuracy = (np.sum(test_labels== predictions) * 100) / len(test_images)\n",
    "\n",
    "  print('%s model accuracy is %.4f%% (Number of test samples=%d)' % (\n",
    "      model_type, accuracy, len(test_images)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b3b57a4-a173-498c-bb87-aee1f9f6569b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float model accuracy is 98.0400% (Number of test samples=10000)\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(tflite_model_file, model_type=\"Float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63817774-d4cc-4f57-b67d-6b4fd3b04e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model accuracy is 98.0100% (Number of test samples=10000)\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(tflite_model_quant_file, model_type=\"Quantized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cd41f3-9e34-4365-be62-ab979220cab8",
   "metadata": {},
   "source": [
    "### Summary - Uniform/Linear Quantization\n",
    "* Quantization executes models in lower precision and is a lossy process\n",
    "* Leads to drop in accuracy of the model : Fast (But inaccurate model is not helpful)\n",
    "* Has amazing benefits for users / performance ==> So we need to get best of both"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53891541-432a-40b4-b3ec-50ec34235550",
   "metadata": {},
   "source": [
    "## QUANTIZATION AWARE TRAINING (QAT)\n",
    "\n",
    "**TF model --> Quantized model --> Fine-tuned quantized model --> TFlite model**\n",
    "\n",
    "### What is it??\n",
    "* Training time technique for improving the accuracy of quantized models\n",
    "* Introduces inference time quantization errors during training so model runs robust parameters around it\n",
    "\n",
    "### How QAT recover lost accuracy??\n",
    "* Make the training path as similar as possible to the inference path **(FP32 --> INT8 --> FP32)**\n",
    "* Mimic the errors experienced by the model during inference in training\n",
    "\n",
    "    * **Mimiking errors, aka Fake Quant**\n",
    "       * Quantize to lower precision, then convert back to float (Introduces the quantize error)\n",
    "       * Both inputs & weights quantized, hence computation exactly mimics the int8 (float values are at the boundary of lower precision)\n",
    "    * **Modelling inferencing path**\n",
    "       * Inference optimizations may fuse ReLU activations or fold batch norm to conv layers\n",
    "       * QAT applies the same transformations to training & adjust fake quant insertions accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db1c7a07-6c3f-4a08-80e6-820928914e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape_1 (Reshape)         (None, 28, 28, 1)         0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 26, 26, 12)        120       \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 13, 13, 12)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 2028)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                20290     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20410 (79.73 KB)\n",
      "Trainable params: 20410 (79.73 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3313 - accuracy: 0.9052 - val_loss: 0.1806 - val_accuracy: 0.9467\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1408 - accuracy: 0.9595 - val_loss: 0.1011 - val_accuracy: 0.9720\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0920 - accuracy: 0.9729 - val_loss: 0.0778 - val_accuracy: 0.9769\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0718 - accuracy: 0.9785 - val_loss: 0.0659 - val_accuracy: 0.9793\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0607 - accuracy: 0.9821 - val_loss: 0.0673 - val_accuracy: 0.9777\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x762e3a7d7910>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load MNIST dataset\n",
    "from tensorflow_model_optimization.python.core.keras.compat import keras\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Normalize the input image so that each pixel value is between 0 to 1.\n",
    "train_images = train_images.astype(np.float32) / 255.0\n",
    "test_images = test_images.astype(np.float32) / 255.0\n",
    "\n",
    "# Define the model architecture\n",
    "model =keras.Sequential([\n",
    "  keras.layers.InputLayer(input_shape=(28, 28)),\n",
    "  keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
    "  keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation='relu'),\n",
    "  keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "  keras.layers.Flatten(),\n",
    "  keras.layers.Dense(10)\n",
    "])\n",
    "print(model.summary())\n",
    "# Train the digit classification model\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "                  from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "model.fit(\n",
    "  train_images,\n",
    "  train_labels,\n",
    "  epochs=5,\n",
    "  validation_data=(test_images, test_labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e54126-6326-49fd-bb59-2daa249c7928",
   "metadata": {},
   "source": [
    "* Quantization aware training to the whole model. All layers are prefixed by \"quant\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f27b0b8f-31f4-4bfa-98d3-4b76264739e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " quantize_layer (QuantizeLa  (None, 28, 28)            3         \n",
      " yer)                                                            \n",
      "                                                                 \n",
      " quant_reshape_1 (QuantizeW  (None, 28, 28, 1)         1         \n",
      " rapperV2)                                                       \n",
      "                                                                 \n",
      " quant_conv2d_1 (QuantizeWr  (None, 26, 26, 12)        147       \n",
      " apperV2)                                                        \n",
      "                                                                 \n",
      " quant_max_pooling2d_1 (Qua  (None, 13, 13, 12)        1         \n",
      " ntizeWrapperV2)                                                 \n",
      "                                                                 \n",
      " quant_flatten_1 (QuantizeW  (None, 2028)              1         \n",
      " rapperV2)                                                       \n",
      "                                                                 \n",
      " quant_dense_1 (QuantizeWra  (None, 10)                20295     \n",
      " pperV2)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20448 (79.88 KB)\n",
      "Trainable params: 20410 (79.73 KB)\n",
      "Non-trainable params: 38 (152.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "quantize_model = tfmot.quantization.keras.quantize_model\n",
    "\n",
    "# q_aware stands for for quantization aware.\n",
    "q_aware_model = quantize_model(model)\n",
    "\n",
    "# `quantize_model` requires a recompile.\n",
    "q_aware_model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "q_aware_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fd58fda1-d1e1-45b1-9ea2-4794113aa1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0616 - accuracy: 0.9822 - val_loss: 0.0543 - val_accuracy: 0.9800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x762e3af04df0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images_subset = train_images[0:1000] # out of 60000\n",
    "train_labels_subset = train_labels[0:1000]\n",
    "\n",
    "q_aware_model.fit(train_images_subset, train_labels_subset,\n",
    "                  batch_size=500, epochs=1, validation_split=0.1)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "39d93823-c038-40bc-b23e-71596b8f4b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline test accuracy: 0.9776999950408936\n",
      "Quant test accuracy: 0.9800999760627747\n"
     ]
    }
   ],
   "source": [
    "_, baseline_model_accuracy = model.evaluate(\n",
    "    test_images, test_labels, verbose=0)\n",
    "\n",
    "_, q_aware_model_accuracy = q_aware_model.evaluate(\n",
    "   test_images, test_labels, verbose=0)\n",
    "\n",
    "print('Baseline test accuracy:', baseline_model_accuracy)\n",
    "print('Quant test accuracy:', q_aware_model_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f5a8831d-fc9c-4db1-90bc-f10e8f62027a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp46o88yx8/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp46o88yx8/assets\n",
      "/home/gk/.local/lib/python3.10/site-packages/tensorflow/lite/python/convert.py:983: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n",
      "W0000 00:00:1729792747.742634    5946 tf_tfl_flatbuffer_helpers.cc:392] Ignored output_format.\n",
      "W0000 00:00:1729792747.742642    5946 tf_tfl_flatbuffer_helpers.cc:395] Ignored drop_control_dependency.\n",
      "2024-10-24 23:29:07.742730: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmp46o88yx8\n",
      "2024-10-24 23:29:07.743725: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2024-10-24 23:29:07.743732: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmp46o88yx8\n",
      "2024-10-24 23:29:07.749330: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2024-10-24 23:29:07.773719: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmp46o88yx8\n",
      "2024-10-24 23:29:07.780560: I tensorflow/cc/saved_model/loader.cc:462] SavedModel load for tags { serve }; Status: success: OK. Took 37832 microseconds.\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "quantized_tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "49f57fb1-2112-4dff-bfa3-d4028e478819",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def evaluate_model(interpreter):\n",
    "  input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "  output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "\n",
    "  # Run predictions on every image in the \"test\" dataset.\n",
    "  prediction_digits = []\n",
    "  for i, test_image in enumerate(test_images):\n",
    "    if i % 1000 == 0:\n",
    "      print('Evaluated on {n} results so far.'.format(n=i))\n",
    "    # Pre-processing: add batch dimension and convert to float32 to match with\n",
    "    # the model's input data format.\n",
    "    test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
    "    interpreter.set_tensor(input_index, test_image)\n",
    "\n",
    "    # Run inference.\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # Post-processing: remove batch dimension and find the digit with highest\n",
    "    # probability.\n",
    "    output = interpreter.tensor(output_index)\n",
    "    digit = np.argmax(output()[0])\n",
    "    prediction_digits.append(digit)\n",
    "\n",
    "  print('\\n')\n",
    "  # Compare prediction results with ground truth labels to calculate accuracy.\n",
    "  prediction_digits = np.array(prediction_digits)\n",
    "  accuracy = (prediction_digits == test_labels).mean()\n",
    "  return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d6880e61-532e-417d-8fae-7f78eacf8387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated on 0 results so far.\n",
      "Evaluated on 1000 results so far.\n",
      "Evaluated on 2000 results so far.\n",
      "Evaluated on 3000 results so far.\n",
      "Evaluated on 4000 results so far.\n",
      "Evaluated on 5000 results so far.\n",
      "Evaluated on 6000 results so far.\n",
      "Evaluated on 7000 results so far.\n",
      "Evaluated on 8000 results so far.\n",
      "Evaluated on 9000 results so far.\n",
      "\n",
      "\n",
      "Quant TFLite test_accuracy: 0.9801\n",
      "Quant TF test accuracy: 0.9800999760627747\n"
     ]
    }
   ],
   "source": [
    "\n",
    "interpreter = tf.lite.Interpreter(model_content=quantized_tflite_model)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "test_accuracy = evaluate_model(interpreter)\n",
    "\n",
    "print('Quant TFLite test_accuracy:', test_accuracy)\n",
    "print('Quant TF test accuracy:', q_aware_model_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ea086d-f8e8-4998-92fa-aec98640b22e",
   "metadata": {},
   "source": [
    "### 4x smaller model from quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e5dc1bc4-5b68-472f-84e9-5bed87cc56a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpzms9p7ci/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpzms9p7ci/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float model in Mb: 0.08073043823242188\n",
      "Quantized model in Mb: 0.02367401123046875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1729792759.097387    5946 tf_tfl_flatbuffer_helpers.cc:392] Ignored output_format.\n",
      "W0000 00:00:1729792759.097395    5946 tf_tfl_flatbuffer_helpers.cc:395] Ignored drop_control_dependency.\n",
      "2024-10-24 23:29:19.097478: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpzms9p7ci\n",
      "2024-10-24 23:29:19.097938: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2024-10-24 23:29:19.097944: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmpzms9p7ci\n",
      "2024-10-24 23:29:19.100598: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2024-10-24 23:29:19.114999: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmpzms9p7ci\n",
      "2024-10-24 23:29:19.119000: I tensorflow/cc/saved_model/loader.cc:462] SavedModel load for tags { serve }; Status: success: OK. Took 21524 microseconds.\n"
     ]
    }
   ],
   "source": [
    "# Create float TFLite model.\n",
    "import tempfile\n",
    "import os\n",
    "float_converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "float_tflite_model = float_converter.convert()\n",
    "\n",
    "# Measure sizes of models.\n",
    "_, float_file = tempfile.mkstemp('.tflite')\n",
    "_, quant_file = tempfile.mkstemp('.tflite')\n",
    "\n",
    "with open(quant_file, 'wb') as f:\n",
    "  f.write(quantized_tflite_model)\n",
    "\n",
    "with open(float_file, 'wb') as f:\n",
    "  f.write(float_tflite_model)\n",
    "\n",
    "print(\"Float model in Mb:\", os.path.getsize(float_file) / float(2**20))\n",
    "print(\"Quantized model in Mb:\", os.path.getsize(quant_file) / float(2**20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3db307-f62f-4e8c-9ce2-cf52f39c440d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
